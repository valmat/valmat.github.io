<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <generator uri="https://gohugo.io/" version="0.152.1">Hugo</generator>
    <title>Algo on Valmat&#39;s Personal Blog</title>
            <link href="https://valmat.ru/tags/algo/" rel="alternate" type="text/html" title="html" />
            <link href="https://valmat.ru/tags/algo/feed.xml" rel="self" type="application/atom+xml" title="atom" />
    <updated>2025-10-23T01:37:11+03:00</updated>
    <id>https://valmat.ru/tags/algo/</id>
        <entry>
            <title>Получение распределений в задачах регрессии</title>
            <link href="https://valmat.ru/posts/2022/09/regress-distr/" rel="alternate" type="text/html"  hreflang="en" />
            <id>https://valmat.ru/posts/2022/09/regress-distr/</id>
            <published>2022-09-20T12:00:00+03:00</published>
            <updated>2022-09-20T12:00:00+03:00</updated>
            <content type="html">
                &lt;p&gt;Приводится алгоритм нахождения функций распределения в качестве решения задачи регрессии.&lt;/p&gt;
&lt;p&gt;В общем виде задачу регрессии можно сформулировать как восстановление зависимости&lt;br&gt;
$\phi: X \to L_1(\Omega)$,&lt;br&gt;
сопоставляющей элементам некоторого фазового пространства $X$ случайную величину $\xi \in L_1(\Omega)$.&lt;/p&gt;
&lt;p&gt;Классический подход к решению задачи регрессии состоит в нахождении среднего значения $E[\phi(x)]$ для каждого $x \in X$.&lt;/p&gt;
&lt;p&gt;В статье предлагается простой алгоритм оценки распределений случайных величин $\phi(x) \in L_1(\Omega)$.&lt;/p&gt;
&lt;p&gt;&lt;a
  class=&#34;gblog-markdown__link&#34;
  href=&#34;https://github.com/valmat&#34;
&gt;GitHub&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;



  &lt;div class=&#34;gblog-toc gblog-toc__level--6&#34;&gt;
    &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#мотивация&#34;&gt;Мотивация&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#описание-подхода&#34;&gt;Описание подхода&lt;/a&gt;
          &lt;ul&gt;
            &lt;li&gt;&lt;a href=&#34;#постановка-задачи&#34;&gt;Постановка задачи&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#построение-модели&#34;&gt;Построение модели&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;&lt;a href=&#34;#ограничения&#34;&gt;Ограничения&lt;/a&gt;&lt;/li&gt;
          &lt;/ul&gt;
        &lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#итоговый-алгоритм&#34;&gt;Итоговый алгоритм&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#валидация&#34;&gt;Валидация&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#эксперименты&#34;&gt;Эксперименты&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#заключение&#34;&gt;Заключение&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
    &lt;hr /&gt;
  &lt;/div&gt;


&lt;hr&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h2 id=&#34;мотивация&#34;
    &gt;
        Мотивация
    &lt;/h2&gt;
    &lt;a data-clipboard-text=&#34;https://valmat.ru/posts/2022/09/regress-distr/#мотивация&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Мотивация&#34; href=&#34;#%d0%bc%d0%be%d1%82%d0%b8%d0%b2%d0%b0%d1%86%d0%b8%d1%8f&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;В анализе данных значительное место занимают два класса задач — задачи классификации и регрессии.&lt;/p&gt;
&lt;p&gt;Так сложилось, что, хотя эти задачи очень похожи, подход к их решению отличается.&lt;/p&gt;
&lt;p&gt;Большинство алгоритмов решения задач &lt;strong&gt;классификации&lt;/strong&gt; позволяют не просто оценить среднее значение $E[\phi(x)]$ для каждого элемента фазового пространства $X$, но и найти плотность распределения.&lt;/p&gt;
&lt;p&gt;Для задач &lt;strong&gt;регрессии&lt;/strong&gt; обычно находят лишь некоторую числовую оценку $\widehat{\phi(x)}$, которая, чаще всего, является средним значением, но не находят плотность распределения.&lt;/p&gt;
&lt;p&gt;Знание плотности распределения даёт гораздо больше возможностей для принятия решений, чем просто оценка среднего.&lt;/p&gt;
&lt;p&gt;Например, для заданной точки $x \in X$ мы можем:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Оценить уверенность прогноза в каждой конкретной точке.&lt;/li&gt;
&lt;li&gt;Найти не среднее, а наиболее вероятное значение случайной величины. Это особенно актуально, если распределение $\phi(x)$ является мультимодальным.&lt;/li&gt;
&lt;li&gt;Определить доверительный интервал возможных значений оценки $\widehat{\phi(x)}$.&lt;/li&gt;
&lt;li&gt;Вычислить любые характеристики распределения, определяемые конкретной задачей и позволяющие более взвешенно и точно принимать решения на основе прогноза модели.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h2 id=&#34;описание-подхода&#34;
    &gt;
        Описание подхода
    &lt;/h2&gt;
    &lt;a data-clipboard-text=&#34;https://valmat.ru/posts/2022/09/regress-distr/#описание-подхода&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Описание подхода&#34; href=&#34;#%d0%be%d0%bf%d0%b8%d1%81%d0%b0%d0%bd%d0%b8%d0%b5-%d0%bf%d0%be%d0%b4%d1%85%d0%be%d0%b4%d0%b0&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h3 id=&#34;постановка-задачи&#34;
    &gt;
        Постановка задачи
    &lt;/h3&gt;
    &lt;a data-clipboard-text=&#34;https://valmat.ru/posts/2022/09/regress-distr/#постановка-задачи&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Постановка задачи&#34; href=&#34;#%d0%bf%d0%be%d1%81%d1%82%d0%b0%d0%bd%d0%be%d0%b2%d0%ba%d0%b0-%d0%b7%d0%b0%d0%b4%d0%b0%d1%87%d0%b8&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;Для простоты опишу подход для одномерной задачи регрессии. В многомерном случае подход аналогичен.&lt;/p&gt;
&lt;p&gt;Имеем некоторое фазовое пространство $X$ и закономерность&lt;/p&gt;
&lt;p&gt;$$
\phi: X \to L_1(\Omega, \mathbb{R})
$$&lt;/p&gt;
&lt;p&gt;$\phi$ сопоставляет случайные величины из $L_1(\Omega)$ точкам фазового пространства $X$.&lt;/p&gt;
&lt;p&gt;Таким образом, мы имеем семейство вероятностных мер $\lbrace P_x\rbrace_{x \in X}$, порождаемых закономерностью $\phi$.&lt;/p&gt;
&lt;p&gt;Нам нужно построить модель, порождающую параметрическое семейство вероятностных мер&lt;/p&gt;
&lt;p&gt;$$
\lbrace Q_{x, \theta}\rbrace_{x \in X, \theta \in \Theta}
$$&lt;/p&gt;
&lt;p&gt;и найти оптимальное значение параметра $\theta_0 \in \Theta$, дающее наилучшее, в некотором смысле, приближение реальных распределений $\lbrace P_x\rbrace_{x \in X}$:&lt;/p&gt;
&lt;p&gt;$$
Q_{x, \theta_0} \sim P_x
$$&lt;/p&gt;
&lt;p&gt;При этом мы располагаем выборкой точек $\lbrace(x_i, y_i)\rbrace_{i=1}^N$, порожденной $N$ независимыми испытаниями: $x_i \in X$, $y_i = \phi(x_i)$.&lt;/p&gt;
&lt;p&gt;$y_i \in L_1(\Omega)$ — являются независимыми случайными величинами. $x_i \in X$, в общем случае, случайными величинами могут и не быть.&lt;/p&gt;
&lt;p&gt;Чтобы понять как строить модель, решающую поставленную задачу, посмотрим как она решается в случае задач классификации.&lt;/p&gt;
&lt;p&gt;В приведенной выше постановке задачи единственным отличием задачи классификации от задачи регрессии является то, что для задач классификации вероятностное пространство $L_1(\Omega)$ является дискретным.&lt;/p&gt;
&lt;p&gt;Когда задача моделирования распределения решается для дискретного $L_1(\Omega)$, т.е. для классификации, реальную плотность распределения приближают функциями вида&lt;/p&gt;
&lt;p&gt;$$
\sum_{k=1}^K  \mathbf{1}_{A_k}
$$&lt;/p&gt;
&lt;p&gt;где $A_k \subseteq \Omega$, $\mathbf{1}_{A}$ — характеристическая функция множества $A$.&lt;/p&gt;
&lt;p&gt;Именно так мы и поступим.&lt;/p&gt;
&lt;p&gt;Только для решения задачи регрессии моделировать лучше не плотность, а функцию распределения. На это есть ряд причин.&lt;/p&gt;
&lt;p&gt;Во-первых, использование функции распределения является более робастным, чем использование плотности.&lt;/p&gt;
&lt;p&gt;Во-вторых, плотность распределения должна удовлетворять свойству $\int_{\mathbb{R}} p(t) dt = 1$. Это свойство может быть сложнее удовлетворить при построении модели, чем соответствующее ограничение на функцию распределения:&lt;/p&gt;
&lt;p&gt;$$
\lim\limits_{t \to -\infty}F(t) = 0, \
\lim\limits_{t \to +\infty}F(t) = 1.
$$&lt;/p&gt;
&lt;hr&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h3 id=&#34;построение-модели&#34;
    &gt;
        Построение модели
    &lt;/h3&gt;
    &lt;a data-clipboard-text=&#34;https://valmat.ru/posts/2022/09/regress-distr/#построение-модели&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Построение модели&#34; href=&#34;#%d0%bf%d0%be%d1%81%d1%82%d1%80%d0%be%d0%b5%d0%bd%d0%b8%d0%b5-%d0%bc%d0%be%d0%b4%d0%b5%d0%bb%d0%b8&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;Вместо привычной для регрессии модели&lt;/p&gt;
&lt;p&gt;$$
M_{\theta}: X \to \mathbb{R}
$$&lt;/p&gt;
&lt;p&gt;и последующего нахождения $\theta$ путём оптимизации, будем строить модель, сразу приближающую функции распределения:&lt;/p&gt;
&lt;p&gt;$$
M_{\theta}: X \to (\mathbb{R} \to [0, 1])
$$&lt;/p&gt;
&lt;p&gt;или, что то же самое:&lt;/p&gt;
&lt;p&gt;$$
M_{\theta}: X \times \mathbb{R} \to [0, 1]
$$&lt;/p&gt;
&lt;p&gt;То есть каждой паре $(x, t)$, $x \in X, t \in \mathbb{R}$, наша модель будет сопоставлять число в интервале $[0, 1]$.&lt;/p&gt;
&lt;p&gt;Например, для нейронных сетей этого легко добиться, поместив сигмоиду последним слоем сети.&lt;/p&gt;
&lt;p&gt;Информация о реальном семействе распределений $\lbrace P_x \rbrace_{x \in X}$, которой мы располагаем, отражена в имеющейся у нас обучающей выборке $\lbrace(x_i, y_i)\rbrace_{i=1}^N$.&lt;/p&gt;
&lt;p&gt;Эта обучающая выборка порождает набор тривиальных функций распределения $\lbrace F_i\rbrace_{i=1}^N$:&lt;/p&gt;
&lt;p&gt;$$
F_i(t) =
\begin{cases}
1, &amp;amp; t \geqslant y_i |\
0, &amp;amp; t &amp;lt; y_i
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;$F_i(t) = 1$, при $t \geqslant y_i$, и $F_i(t) = 0$, при $t &amp;lt; y_i$.&lt;/p&gt;
&lt;p&gt;Чтобы уйти от задачи построения модели, аппроксимирующей выборку функций, к хорошо изученной задаче построения модели, аппроксимирующей выборку точек, перейдем от выборки $\lbrace(x_i, y_i)\rbrace_{i=1}^N$ к выборке&lt;/p&gt;
&lt;p&gt;$$
\bigcup\limits_{i=1}^N \lbrace(x_i, t_j, F_i(t_j))\rbrace_{j \in J_i}
$$&lt;/p&gt;
&lt;p&gt;Для этого для каждого $i = 1&amp;hellip;N$ случайным образом подберём числа $t_j$ для $j \in J_i$ из некоторого диапазона допустимых значений $y$.&lt;/p&gt;
&lt;p&gt;Таким образом, мы снова приходим к классической задаче регрессии, но фазовым пространством для нее будет не исходное пространство $X$, а пространство $X \times Y$, где $Y \subseteq \mathbb{R}$ — множество допустимых значений $y$.&lt;/p&gt;
&lt;p&gt;То есть мы получили обычную задачу регрессии для выборки $\lbrace(z_k, u_k)\rbrace_{k=1}^M$, где&lt;br&gt;
$z_k = (x_l, t_s)$, а $u_k = F_l(t_s) \in [0, 1]$, для некоторых $l$ и $s$.&lt;/p&gt;
&lt;p&gt;Для решения этой задачи можно применить любой алгоритм обучения с учителем из арсенала методов решения задач регрессии.&lt;/p&gt;
&lt;hr&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h3 id=&#34;ограничения&#34;
    &gt;
        Ограничения
    &lt;/h3&gt;
    &lt;a data-clipboard-text=&#34;https://valmat.ru/posts/2022/09/regress-distr/#ограничения&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Ограничения&#34; href=&#34;#%d0%be%d0%b3%d1%80%d0%b0%d0%bd%d0%b8%d1%87%d0%b5%d0%bd%d0%b8%d1%8f&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;Поскольку описанный выше способ моделирует построение функций распределения, наша модель должна удовлетворять некоторым дополнительным ограничениям.&lt;/p&gt;
&lt;p&gt;Пусть&lt;/p&gt;
&lt;p&gt;$$
M_{\theta}: X \times Y \to [0, 1], \theta \in \Theta
$$&lt;/p&gt;
&lt;p&gt;— параметрическое семейство моделей, и $\theta_0$ — оптимальная оценка параметра, дающая приближение реального семейства распределений $\lbrace P_x\rbrace_{x \in X}$, и&lt;/p&gt;
&lt;p&gt;$$
M = \lim\limits_{\theta \to \theta_0, \theta \in \Theta} M_{\theta}
$$&lt;/p&gt;
&lt;p&gt;— итоговая модель.&lt;/p&gt;
&lt;p&gt;Тогда должны быть выполнены требования:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Для каждого $x \in X$ $M(x, \cdot): t \to [0, 1] $ — является функцией некоторого распределения.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;То есть должны быть удовлетворены следующие условия:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\lim\limits_{t \to -\infty} M(x ,t) = 0$,&lt;br&gt;
$\lim\limits_{t \to +\infty} M(x ,t) = 1$&lt;/li&gt;
&lt;li&gt;$t_1 \leqslant t_2 \Rightarrow M(x, t_1) \leqslant M(x, t_2)$&lt;/li&gt;
&lt;li&gt;$M(x, t) \in [0, 1],, \forall t \in \mathbb{R}$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Все эти условия, в общем случае, не обязаны выполняться по построению моделей $M_{\theta}$ способом, описанным выше.&lt;/p&gt;
&lt;p&gt;Условие (3) может быть удовлетворено путём наложения ограничений на саму модель. Например, для нейронных сетей можно последним слоем разместить сигмоиду.&lt;/p&gt;
&lt;p&gt;Практика показала, что для правильно построенной модели при достаточном объеме обучающей выборки условия (1) и (2) будут выполнены автоматически. Но эти условия должны быть вынесены на этап валидации в качестве дополнительного обязательного критерия правильности построения модели.&lt;/p&gt;
&lt;hr&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h2 id=&#34;итоговый-алгоритм&#34;
    &gt;
        Итоговый алгоритм
    &lt;/h2&gt;
    &lt;a data-clipboard-text=&#34;https://valmat.ru/posts/2022/09/regress-distr/#итоговый-алгоритм&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Итоговый алгоритм&#34; href=&#34;#%d0%b8%d1%82%d0%be%d0%b3%d0%be%d0%b2%d1%8b%d0%b9-%d0%b0%d0%bb%d0%b3%d0%be%d1%80%d0%b8%d1%82%d0%bc&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;Кратко опишем алгоритм.&lt;/p&gt;
&lt;p&gt;Дана обучающая выборка $\lbrace(x_i, y_i)\rbrace_{i=1}^N$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Находим диапазон допустимых значений $Y$.&lt;/strong&gt;&lt;br&gt;
Например,
$$
Y = [\min\limits_{i} y_i - a, \max\limits_{i} y_i + a],
$$
где $a$ — некоторое число, подбираемое исследователем.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Для каждой пары $(x_i, y_i)$ случайно генерируем набор точек $\lbrace t_j\rbrace_{j \in J_i} \subseteq Y$.&lt;/strong&gt;&lt;br&gt;
$\lbrace t_j\rbrace$ нужно генерировать так, чтобы было достаточно точек, лежащих левее $y_i$ и достаточно точек, лежащих правее $y_i$.&lt;br&gt;
Можно задать разбиение $\lbrace t_j\rbrace_{j \in J} \subseteq Y$ одинаковое для всех $i$, но тогда мы теряем разнообразие обучающей выборки в тех случаях, когда $(x_i, y_i)$ и $(x_k, y_k)$ — близкие, но не совпадающие точки.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;После того, как точки $\lbrace t_j\rbrace_{j \in J_i}$ сгенерированы, генерируем новую обучающую выборку, как объединение выборок:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;$$
\bigcup\limits_{i=1}^N \lbrace(x_i, t_j, u_{i j})\rbrace_{j \in J_i}
$$&lt;/p&gt;
&lt;p&gt;где&lt;/p&gt;
&lt;p&gt;$$
u_{i j} =
\begin{cases}
1, &amp;amp; t_j \geqslant y_i |\
0, &amp;amp; t_j &amp;lt; y_i
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;$u_{i j} = 1$, при $t_j \geqslant y_i$, и $u_{i j} = 0$, при $t_j &amp;lt; y_i$.&lt;/p&gt;
&lt;p&gt;Для удобства обозначим $z_{i j} = (x_i, t_j)$.&lt;br&gt;
$z_{i j}$ будут лежать в области определения нашей модели, т.е. будут являться признаками, а $u_{i j}$ в области значений, т.е. будут являться таргетами.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Новую полученную выборку лучше случайно перемешать, перед тем как приступать к обучению модели.&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Строим модель обучения с учителем на обучающей выборке $\lbrace(z_{i j}, u_{i j})\rbrace$ как для обычной задачи регрессии.&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Проводим валидацию модели.&lt;/strong&gt;&lt;br&gt;
В частности, на удовлетворение условия того, что $M(x, t)$ является функцией распределения по $t$ для каждого $x \in X$, т.е. проверяем (1), (2), (3).&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h2 id=&#34;валидация&#34;
    &gt;
        Валидация
    &lt;/h2&gt;
    &lt;a data-clipboard-text=&#34;https://valmat.ru/posts/2022/09/regress-distr/#валидация&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Валидация&#34; href=&#34;#%d0%b2%d0%b0%d0%bb%d0%b8%d0%b4%d0%b0%d1%86%d0%b8%d1%8f&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;Как и для обычных задач регрессии, невозможно дать какие-то универсальные критерии оценки качества построения модели. Но можно дать несколько рекомендаций, позволяющих оценить это качество.&lt;/p&gt;
&lt;p&gt;В любом случае, модель $M(x, t)$ должна быть функцией распределения по $t$ для всех $x \in X$. Если ограничения (1), (2), (3) не выполнены для $M(x, \cdot)$, то такую модель следует отвергнуть как некачественную.&lt;/p&gt;
&lt;p&gt;Сам алгоритм по построению является обычной задачей регрессии. И к его результатам применимы все метрики качества, применяемые к задачам регрессии.&lt;/p&gt;
&lt;p&gt;Для получения этих метрик тестовую выборку $\lbrace(x_i, y_i)\rbrace$ нужно привести к виду $\lbrace(z_{i j}, u_{i j})\rbrace$ тем же способом, что и обучающую.&lt;/p&gt;
&lt;p&gt;Кроме того, мы можем перейти на уровень исходных данных и для каждой $x_i$ из тестовой выборки посчитать среднее значение $\widehat{y_i}$ как&lt;/p&gt;
&lt;p&gt;$$
\widehat{y_i} = \int\limits_{t \in Y} t, dM(x_i, t)
$$&lt;/p&gt;
&lt;p&gt;Таким образом, мы можем оценивать качество модели так, как если бы мы не строили распределения, а решали обычную задачу регрессии.&lt;/p&gt;
&lt;p&gt;Замечу, что в некоторых случаях вместо оценки среднего $\widehat{y}$ более уместным будет оценивать наиболее вероятное значение $y$:&lt;/p&gt;
&lt;p&gt;$$
\widehat{y_i} =
\arg\max \limits_{t \in Y}
\frac{\partial M(x_i, t)}{\partial t}
$$&lt;/p&gt;
&lt;p&gt;В целом, подход с моделированием распределений вместо моделирования значений даёт не меньше, а даже больше способов оценки качества модели.&lt;/p&gt;
&lt;hr&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h2 id=&#34;эксперименты&#34;
    &gt;
        Эксперименты
    &lt;/h2&gt;
    &lt;a data-clipboard-text=&#34;https://valmat.ru/posts/2022/09/regress-distr/#эксперименты&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Эксперименты&#34; href=&#34;#%d1%8d%d0%ba%d1%81%d0%bf%d0%b5%d1%80%d0%b8%d0%bc%d0%b5%d0%bd%d1%82%d1%8b&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;В качестве базовой закономерности возьмём функцию&lt;/p&gt;
&lt;p&gt;$$
f(x) = 1 - x^2 + \frac{3}{2} x - \sin(2 \pi x^2)
$$&lt;/p&gt;
&lt;p&gt;на отрезке $x \in [0, 1]$.&lt;/p&gt;
&lt;p&gt;Моделируем&lt;br&gt;
&lt;a
  class=&#34;gblog-markdown__link&#34;
  href=&#34;https://github.com/valmat/regress_distr/blob/master/experements.ipynb&#34;
&gt;Исходный код экспериментов&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Закономерность определяется выражением выше плюс нормальный шум $\mathcal{N}(f(x), \sigma(x))$, где&lt;/p&gt;
&lt;p&gt;$$
\sigma(x) = 0.05 + \frac{x}{2}
$$&lt;/p&gt;
&lt;p&gt;То есть для каждой точки $x \in [0, 1]$ нашего фазового пространства значения соответствующей случайной величины, определяемой моделируемой закономерностью, распределены по закону:&lt;/p&gt;
&lt;p&gt;$$
\mathcal{N}(f(x), \sigma(x))
$$&lt;/p&gt;
&lt;p&gt;&lt;em&gt;На рисунках ниже:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;(a) моделируемая закономерность&lt;/li&gt;
&lt;li&gt;(b) решение обычной задачи регрессии&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img
  src=&#34;ex1_pic2.png&#34;
  alt=&#34;Моделируемая закономерность и решение обычной задачи регрессии&#34;
  
/&gt;
&lt;img
  src=&#34;ex1_pic4.png&#34;
  alt=&#34;Функция распределения и плотность распределения&#34;
  
/&gt;
&lt;img
  src=&#34;ex1_pic6.png&#34;
  alt=&#34;Средние и наиболее вероятные значения, модельная функция распределения $F(x, y)$&#34;
  
/&gt;&lt;/p&gt;
&lt;p&gt;Все функции распределения для всех точек:
&lt;img
  src=&#34;ex1_pic7.png&#34;
  alt=&#34;Полная функция распределения&#34;
  
/&gt;&lt;/p&gt;
&lt;p&gt;Если решать обычную задачу регрессии с помощью нейронной сети, то можно увидеть, что выдаваемые моделью ответы будут довольно хорошо ложиться на средние значения, как это и ожидалось.&lt;/p&gt;
&lt;p&gt;Нахождение распределений методом, описанным в настоящей статье, тоже даёт хорошие результаты.&lt;/p&gt;
&lt;hr&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h2 id=&#34;заключение&#34;
    &gt;
        Заключение
    &lt;/h2&gt;
    &lt;a data-clipboard-text=&#34;https://valmat.ru/posts/2022/09/regress-distr/#заключение&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Заключение&#34; href=&#34;#%d0%b7%d0%b0%d0%ba%d0%bb%d1%8e%d1%87%d0%b5%d0%bd%d0%b8%d0%b5&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;На практике, при достаточном объеме обучающей выборки, непрерывные алгоритмы машинного обучения, такие как нейронные сети, дают хорошее приближение для функций распределения.&lt;/p&gt;
&lt;p&gt;В обучающей выборке могут быть образцы с близкими значениями признака $x$, но различными значениями таргета $y$. Все они вносят вклад в обучение функций распределения.&lt;/p&gt;
&lt;p&gt;Эксперименты и практический опыт показывают, что ограничения, накладываемые на функцию распределения, удовлетворяются.&lt;/p&gt;
&lt;p&gt;Прогнозирование распределений вместо прогнозирования средних значений даёт намного более богатые возможности для принятия решений.&lt;/p&gt;
&lt;p&gt;Моделирование распределений вместо моделирования значений требует меньше дополнительных и часто невыполнимых ограничений.&lt;/p&gt;
&lt;p&gt;Например, если рассмотреть решение одной и той же задачи моделированием распределений&lt;br&gt;
$M_{\theta}(x, t) \in [0, 1]$ и моделированием значений $R_{\theta}(x) \in \mathbb{R}$, то применение МНК, то есть MSE в качестве функции потерь, для $R_{\theta}(x)$ равносильно предположению&lt;/p&gt;
&lt;p&gt;$$
M_{\theta}(x, t) \sim \mathcal{N}(t , \widehat{y}, \sigma)
$$&lt;/p&gt;
&lt;p&gt;что, чаще всего, неверно.&lt;/p&gt;
&lt;p&gt;Конечно, для нахождения оптимальной модели $M_{\theta_0}(x, t)$ мы тоже вынуждены сделать некоторое предположение на вид распределения ошибки $M_{\theta_0}(x, t) - \widehat{u}$  но это предположение ограничивает нас менее жёстко.&lt;/p&gt;
&lt;p&gt;Платой за преимущества, даваемые моделью, предсказывающей распределения, является необходимость обучать более ёмкую модель. А следовательно, более медленная скорость сходимости по сравнению с классическим подходом.&lt;/p&gt;
&lt;p&gt;Действительно, нам нужно выучить не просто среднее, но и дополнительную информацию о форме распределения.&lt;/p&gt;
&lt;p&gt;Кроме того, мы вынуждены искусственно увеличить объём обучающей выборки, выполняя пополнение её таким образом, как это было описано выше.&lt;/p&gt;
&lt;p&gt;Это дополнительно приводит к замедлению обучения и требует больше вычислительных ресурсов.&lt;/p&gt;

            </content>    
                                <category scheme="https://valmat.ru/tags/AI" term="AI" label="AI" /> 
                                <category scheme="https://valmat.ru/tags/algo" term="algo" label="algo" />
        </entry>
        <entry>
            <title>Определение угла наклона текста на сканированных изображениях</title>
            <link href="https://valmat.ru/posts/2022/02/slope-detection/" rel="alternate" type="text/html"  hreflang="en" />
            <id>https://valmat.ru/posts/2022/02/slope-detection/</id>
            <published>2022-02-08T12:00:00+03:00</published>
            <updated>2022-02-08T12:00:00+03:00</updated>
            <content type="html">
                &lt;p&gt;При оптическом распознавании текста на сканированных документах качество распознавания зависит от того, наклонён ли текст в документе. У выровненных документов качество распознавания заметно лучше. Соответственно, возникает практическая необходимость в средствах автоматического выравнивания угла наклона текста.&lt;/p&gt;
&lt;p&gt;В статье предлагается простой, универсальный и достаточно эффективный алгоритм выравнивания наклона текста, основанный на идее минимизации средней энтропии строк и столбцов растрового изображения.&lt;/p&gt;
&lt;hr&gt;



  &lt;div class=&#34;gblog-toc gblog-toc__level--6&#34;&gt;
    &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#идея&#34;&gt;Идея&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#эксперимент&#34;&gt;Эксперимент&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#алгоритм&#34;&gt;Алгоритм&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#ссылки&#34;&gt;Ссылки&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
    &lt;hr /&gt;
  &lt;/div&gt;


&lt;hr&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h2 id=&#34;идея&#34;
    &gt;
        Идея
    &lt;/h2&gt;
    &lt;a data-clipboard-text=&#34;https://valmat.ru/posts/2022/02/slope-detection/#идея&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Идея&#34; href=&#34;#%d0%b8%d0%b4%d0%b5%d1%8f&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;Базовая идея алгоритма состоит в том, что при повороте текста на сканированном изображении средняя, по строкам и столбцам, энтропия распределения пикселей должна возрасти.&lt;/p&gt;
&lt;p&gt;Предположим, нам дан чёрно-белый скан изображения. То есть, каждый пиксель может принимать только два значения: 0 или 1. Как известно, энтропия равномерного распределения максимальна. Если изображение повёрнуто, то в среднем, распределение чёрных и белых пикселей по строкам (и столбцам) будет ближе к равномерному, чем у неповёрнутого изображения. У выровненного изображения распределение пикселей в среднем должно быть менее равномерным.&lt;/p&gt;
&lt;p&gt;&lt;img
  src=&#34;img_1.png&#34;
  alt=&#34;Пример изображения&#34;
  
/&gt;&lt;/p&gt;
&lt;p&gt;Гипотеза состоит в том, чтобы вычислить среднюю по строкам и столбцам энтропию распределения пикселей для разных углов поворота и найти такой угол, при котором эта усреднённая энтропия примет минимальное значение.&lt;/p&gt;
&lt;p&gt;Для проверки этой гипотезы в интернете был собран набор данных различных видов сканированных изображений, после чего предположения были проверены экспериментально.&lt;/p&gt;
&lt;p&gt;Предложенный подход работает и позволяет абсолютно точно определить угол поворота в 83% случаев и с точностью до 1° — в 98% случаев.&lt;/p&gt;
&lt;p&gt;Хотя, на первый взгляд, энтропия Шеннона хорошо подходит для этой задачи, было бы разумно не ограничиваться только ей, а рассмотреть весь спектр энтропий Реньи. И с учётом полученных результатов, а также вычислительной сложности, выбрать оптимальное значение параметра энтропии Реньи.&lt;/p&gt;
&lt;p&gt;Энтропия Реньи вычисляется по формуле:&lt;/p&gt;
&lt;p&gt;$$
R_{\alpha} = \frac{1}{1-\alpha} \log\left( \sum_{i=1}^{n}p_i^{\alpha} \right),
$$&lt;/p&gt;
&lt;p&gt;где $p_i$ — вероятности, соответствующие распределению (в нашем случае — частоты чёрных и белых пикселей).&lt;/p&gt;
&lt;p&gt;В случае $\alpha = 1$ это превращается в энтропию Шеннона:&lt;/p&gt;
&lt;p&gt;$$
R_{1} = H = - \sum_{i=1}^{n} p_i \log(p_i)
$$&lt;/p&gt;
&lt;hr&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h2 id=&#34;эксперимент&#34;
    &gt;
        Эксперимент
    &lt;/h2&gt;
    &lt;a data-clipboard-text=&#34;https://valmat.ru/posts/2022/02/slope-detection/#эксперимент&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Эксперимент&#34; href=&#34;#%d1%8d%d0%ba%d1%81%d0%bf%d0%b5%d1%80%d0%b8%d0%bc%d0%b5%d0%bd%d1%82&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;Для проведения эксперимента был собран набор различных документов в сети Интернет. Каждое изображение из набора было повёрнуто на случайный угол в интервале от -45° до 45°, после чего был вычислен угол поворота с помощью предложенного алгоритма.&lt;/p&gt;
&lt;p&gt;&lt;img
  src=&#34;img_2.png&#34;
  alt=&#34;Пример повёрнутого изображения&#34;
  
/&gt;&lt;/p&gt;
&lt;p&gt;В таблице ниже представлены результаты для различных значений параметра энтропии Реньи $\alpha$:&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Параметр энтропии Реньи $\alpha$&lt;/th&gt;
          &lt;th&gt;1/8&lt;/th&gt;
          &lt;th&gt;1/4&lt;/th&gt;
          &lt;th&gt;1/2&lt;/th&gt;
          &lt;th&gt;3/4&lt;/th&gt;
          &lt;th&gt;1&lt;/th&gt;
          &lt;th&gt;2&lt;/th&gt;
          &lt;th&gt;5&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Среднее абсолютное отклонение&lt;/td&gt;
          &lt;td&gt;0.498&lt;/td&gt;
          &lt;td&gt;0.299&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;0.211&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;0.283&lt;/td&gt;
          &lt;td&gt;0.240&lt;/td&gt;
          &lt;td&gt;5.827&lt;/td&gt;
          &lt;td&gt;41.099&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Доля полных совпадений (точность)&lt;/td&gt;
          &lt;td&gt;0.822&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;0.834&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;0.828&lt;/td&gt;
          &lt;td&gt;0.815&lt;/td&gt;
          &lt;td&gt;0.805&lt;/td&gt;
          &lt;td&gt;0.641&lt;/td&gt;
          &lt;td&gt;0.009&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Доля совпадений с точностью до 1°&lt;/td&gt;
          &lt;td&gt;0.942&lt;/td&gt;
          &lt;td&gt;0.969&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;0.980&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;0.982&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;0.983&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;0.822&lt;/td&gt;
          &lt;td&gt;0.015&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Доля совпадений с точностью до 2°&lt;/td&gt;
          &lt;td&gt;0.967&lt;/td&gt;
          &lt;td&gt;0.983&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;0.991&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;0.993&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;0.994&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;0.841&lt;/td&gt;
          &lt;td&gt;0.015&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Всего было обработано 1665 документов.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Выводы из таблицы:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Наименьшее среднее абсолютное отклонение достигается при $\alpha = \frac{1}{2}$.&lt;/li&gt;
&lt;li&gt;Наилучшая точность (доля полных совпадений) — при $\alpha = \frac{1}{4}$.&lt;/li&gt;
&lt;li&gt;Наилучшая приемлемая точность (доля совпадений с точностью до 1° и до 2°) — при $\alpha \in {1, \frac{3}{4}, \frac{1}{2}}$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Если рассматривать методику как часть комплекса оптического распознавания документов, то наилучшим значением оказывается $\alpha = \frac{1}{2}$.&lt;/p&gt;
&lt;p&gt;При $\alpha = \frac{1}{2}$ среднее абсолютное отклонение составит всего $0.211^\circ$. При этом достигается оптимальная доля совпадений с точностью до 1°.&lt;/p&gt;
&lt;p&gt;Есть ещё одна причина выбрать $\alpha = \frac{1}{2}$: при этом значении достигается оптимальная вычислительная сложность.&lt;/p&gt;
&lt;p&gt;Ниже представлены результаты бенчмарка многократного вычисления энтропий для различных значений параметра $\alpha$:&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;$\alpha$&lt;/th&gt;
          &lt;th&gt;nanoseconds&lt;/th&gt;
          &lt;th&gt;miliseconds&lt;/th&gt;
          &lt;th&gt;% of Shannon&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;10249895206&lt;/td&gt;
          &lt;td&gt;10249&lt;/td&gt;
          &lt;td&gt;100&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;1/2&lt;/td&gt;
          &lt;td&gt;8677368472&lt;/td&gt;
          &lt;td&gt;8677&lt;/td&gt;
          &lt;td&gt;84.66&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;1/4&lt;/td&gt;
          &lt;td&gt;10421639934&lt;/td&gt;
          &lt;td&gt;10421&lt;/td&gt;
          &lt;td&gt;120.1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;1/8&lt;/td&gt;
          &lt;td&gt;13235709810&lt;/td&gt;
          &lt;td&gt;13235&lt;/td&gt;
          &lt;td&gt;127&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;3/4&lt;/td&gt;
          &lt;td&gt;11403406522&lt;/td&gt;
          &lt;td&gt;11403&lt;/td&gt;
          &lt;td&gt;86.16&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;7245386547&lt;/td&gt;
          &lt;td&gt;7245&lt;/td&gt;
          &lt;td&gt;63.54&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;5&lt;/td&gt;
          &lt;td&gt;7771674801&lt;/td&gt;
          &lt;td&gt;7771&lt;/td&gt;
          &lt;td&gt;107.26&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;10&lt;/td&gt;
          &lt;td&gt;10809162384&lt;/td&gt;
          &lt;td&gt;10809&lt;/td&gt;
          &lt;td&gt;139.08&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Из таблицы видно, что среди подходящих значений $\alpha$ наилучшая производительность достигается при $\alpha = 1/2$.&lt;/p&gt;
&lt;hr&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h2 id=&#34;алгоритм&#34;
    &gt;
        Алгоритм
    &lt;/h2&gt;
    &lt;a data-clipboard-text=&#34;https://valmat.ru/posts/2022/02/slope-detection/#алгоритм&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Алгоритм&#34; href=&#34;#%d0%b0%d0%bb%d0%b3%d0%be%d1%80%d0%b8%d1%82%d0%bc&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Замечание:&lt;/strong&gt;&lt;br&gt;
Предлагаемый ниже алгоритм (&lt;a
  class=&#34;gblog-markdown__link&#34;
  href=&#34;https://github.com/valmat/rotate_detection&#34;
&gt;исходный код на GitHub&lt;/a&gt;) предполагает, что для определения угла поворота мы используем бинарное чёрно-белое растровое изображение, в котором каждый пиксель может принимать два значения: 0 или 1.&lt;/p&gt;
&lt;p&gt;Для применения алгоритма необходимо получить бинаризованную копию изображения.&lt;/p&gt;
&lt;p&gt;Я реализовал алгоритм с применением библиотеки &lt;em&gt;libleptonica&lt;/em&gt; (используется в TesseractOCR). Для этого использовал последовательное преобразование &lt;code&gt;pixContrastTRC&lt;/code&gt; с &lt;code&gt;contrast_factor = 1.0&lt;/code&gt; и затем &lt;code&gt;pixConvertTo1&lt;/code&gt; с &lt;code&gt;threshold = 170&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Пусть $h$ — высота, $w$ — ширина исходного изображения. Пусть $d = \sqrt{w^2 + h^2}$ — длина диагонали.&lt;/p&gt;
&lt;p&gt;Будем поворачивать изображение на угол $\phi$ относительно центра изображения и считать среднюю энтропию по строкам и столбцам. Чтобы не выйти за границы, мысленно расширим полотно до размеров $d \times d$.&lt;/p&gt;
&lt;p&gt;Определим:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$x_{from} = \frac{d}{2} - \frac{h |\sin(\phi)| + w |\cos(\phi)|}{2}$,&lt;/li&gt;
&lt;li&gt;$x_{to}   = d - x_{from}$,&lt;/li&gt;
&lt;li&gt;$y_{from} = \frac{d}{2} - \frac{h |\cos(\phi)| + w |\sin(\phi)|}{2}$,&lt;/li&gt;
&lt;li&gt;$y_{to}   = d - y_{from}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;где $x_{from}$ и $x_{to}$ — границы по ширине, $y_{from}$ и $y_{to}$ — по высоте.&lt;/p&gt;
&lt;p&gt;Нужно посчитать среднюю энтропию по строкам (и аналогично по столбцам).&lt;/p&gt;
&lt;p&gt;Пусть $V(x, y)$ — цвет пикселя $(x, y)$ (0 или 1), $R({p, q})$ — энтропия распределения ${p, q}$.&lt;/p&gt;
&lt;p&gt;Алгоритм расчёта средней энтропии $S_{\phi}$ для угла $\phi$ (по строкам):&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code class=&#34;language-pseudo&#34; data-lang=&#34;pseudo&#34;&gt;S_phi = 0
for y in y_from .. y_to:
    b = 0  // количество чёрных пикселей в строке
    for x in x_from .. x_to:
        x_tilde = x - d/2
        y_tilde = y - d/2
        x&amp;#39; = x_tilde * cos(phi) - y_tilde * sin(phi) + w/2
        y&amp;#39; = x_tilde * sin(phi) + y_tilde * cos(phi) + h/2
        if x&amp;#39; &amp;gt;= 0 and x&amp;#39; &amp;lt; w and y&amp;#39; &amp;gt;= 0 and y&amp;#39; &amp;lt; h:
            b = b + V(x&amp;#39;, y&amp;#39;)
    p = b / d
    q = 1 - p
    S_phi = S_phi + R({p, q}) / d
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Полученное значение $S_\phi$ — средняя энтропия для угла поворота $\phi$.&lt;/p&gt;
&lt;p&gt;Для нахождения искомого угла поворота $\phi_0$ нужно найти минимум $S_\phi$:&lt;/p&gt;
&lt;p&gt;$$
\phi_0 = -\arg\min_\phi S_\phi
$$&lt;/p&gt;
&lt;p&gt;Знак минус берётся потому, что для выравнивания изображения нужно повернуть его в обратную сторону.&lt;/p&gt;
&lt;hr&gt;
&lt;div class=&#34;flex align-center gblog-post__anchorwrap&#34;&gt;
    &lt;h2 id=&#34;ссылки&#34;
    &gt;
        Ссылки
    &lt;/h2&gt;
    &lt;a data-clipboard-text=&#34;https://valmat.ru/posts/2022/02/slope-detection/#ссылки&#34; class=&#34;gblog-post__anchor clip flex align-center&#34; aria-label=&#34;Anchor Ссылки&#34; href=&#34;#%d1%81%d1%81%d1%8b%d0%bb%d0%ba%d0%b8&#34;&gt;
        &lt;svg class=&#34;gblog-icon gblog_link&#34;&gt;&lt;use xlink:href=&#34;#gblog_link&#34;&gt;&lt;/use&gt;&lt;/svg&gt;
    &lt;/a&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a
  class=&#34;gblog-markdown__link&#34;
  href=&#34;https://ru.wikipedia.org/wiki/%d0%ad%d0%bd%d1%82%d1%80%d0%be%d0%bf%d0%b8%d1%8f_%d0%a0%d0%b5%d0%bd%d1%8c%d0%b8&#34;
&gt;WikiPedia: Энтропия Реньи (ru)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a
  class=&#34;gblog-markdown__link&#34;
  href=&#34;https://github.com/valmat/rotate_detection&#34;
&gt;Исходный код алгоритма на GitHub (C++)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a
  class=&#34;gblog-markdown__link&#34;
  href=&#34;https://gist.github.com/valmat/6a737cc3783449c4f7a829e77c77393e&#34;
&gt;Исходный код бенчмарка производительности энтропий на C++&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a
  class=&#34;gblog-markdown__link&#34;
  href=&#34;/posts/2022/02/entropy-benchmark/&#34;
&gt;Бенчмарк производительности энтропий&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

            </content>    
                                <category scheme="https://valmat.ru/tags/cpp" term="cpp" label="cpp" /> 
                                <category scheme="https://valmat.ru/tags/AI" term="AI" label="AI" /> 
                                <category scheme="https://valmat.ru/tags/algo" term="algo" label="algo" />
        </entry>
</feed>
