---
title: "Lost in the Middle. Перевод знаменитой статьи"
date: 2025-08-30T22:38:23+03:00
draft: true
tags: ["archive"]
---

Ниже представлен перевод знаменитой [статьи Lost in the Middle](https://arxiv.org/abs/2307.03172) о том, что номинальная длина контекстного окна -- это совсем не то же самое, что и эффективная.

Ссылки:

- [PDF оригинальной статьи](Lost_in_the_Middle-2307.03172v3-en.pdf)
- [PDF перевода статьи](Lost_in_the_Middle-2307.03172v3-ru.pdf)
- Источник: [https://arxiv.org/abs/2307.03172](https://arxiv.org/abs/2307.03172)


# Потерянные в середине: как языковые модели используют длинные контексты  
*Lost in the Middle: How Language Models Use Long Contexts*

**Авторы:**  
Nelson F. Liu\*, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang  
\*Работа частично выполнена в качестве стажёра в Samaya AI.  


---

## Аннотация

Хотя современные языковые модели могут принимать длинные контексты в качестве входных данных, относительно мало известно о том, насколько хорошо они *используют* эти длинные контексты. Мы анализируем производительность языковых моделей в двух задачах, требующих идентификации релевантной информации в их входных контекстах: многодокументный вопросно-ответный анализ и извлечение ключевых значений.

Мы обнаруживаем, что производительность может значительно ухудшаться при изменении позиции релевантной информации, что указывает на то, что текущие языковые модели не могут надежно использовать информацию в длинных входных контекстах. В частности, производительность часто максимальна, когда релевантная информация находится в начале или в конце входного контекста, и существенно падает, если нужная информация расположена в середине. Наш анализ даёт лучшее понимание того, как языковые модели используют свой входной контекст, и предлагает новые протоколы оценки для будущих моделей с длинным контекстом.

---

## Введение

![U-образная кривая производительности](figures/figure1.png)  
*Изменение местоположения релевантной информации в контексте входных данных языковой модели приводит к U-образной кривой производительности — модели лучше используют релевантную информацию, которая находится в самом начале (эффект первичности) или в конце её входного контекста (эффект недавности), а производительность значительно ухудшается, когда модели должны получать доступ и использовать информацию, расположенную в середине.*

Языковые модели стали важным и гибким строительным блоком в различных языковых технологиях, включая чат-боты, поиск, суммаризацию и совместное написание текстов. Эти модели работают с задачами через подсказки: вся спецификация задачи и данные для обработки форматируются как текстовый входной контекст, а модель возвращает сгенерированный текст.

Контексты могут содержать тысячи токенов, особенно при работе с длинными документами (например, юридическими или научными) или при дополнении внешней информацией (например, релевантными документами из поиска или результатами запросов к базе данных).

Обработка таких случаев требует, чтобы языковые модели успешно работали с длинными последовательностями. Однако, несмотря на появление моделей с большими оконными контекстами (4096, 32K, 100K токенов), остаётся неясным, как они используют свои длинные входные контексты на практике.

Мы эмпирически исследуем этот вопрос с помощью контролируемых экспериментов на различных современных открытых (MPT-30B-Instruct, LongChat-13B) и закрытых (GPT-3.5-Turbo, Claude-1.3) языковых моделях. В экспериментах мы варьируем размер входного контекста и положение релевантной информации, чтобы изучить влияние этих факторов на производительность.

**Ключевое наблюдение:**  
Если языковые модели могут надёжно использовать информацию из длинных входных контекстов, их производительность должна быть *минимально чувствительна* к положению релевантной информации.

### Кратко о задачах

- **Многодокументный вопросно-ответный анализ:** Модель должна найти релевантную информацию среди предоставленных документов и использовать её для ответа на вопрос. Мы варьируем (i) длину входного контекста (количество документов) и (ii) положение релевантного документа (в начале, середине или конце).
- **Извлечение ключевых значений:** Синтетическая задача, где модель должна найти значение по ключу в длинном списке пар ключ-значение (например, JSON-объект).

### Основные выводы

- **U-образная кривая:** Производительность моделей максимальна, когда релевантная информация в начале или в конце контекста, и минимальна — в середине.
- **Модели с расширенным контекстом не обязательно лучше используют длинные входные контексты.**
- **Даже простое извлечение совпадающих токенов из середины длинного контекста затруднено для многих моделей.**

### Почему так происходит?

- **Архитектура:** Кодер-декодер-модели устойчивы к изменению позиции релевантной информации, но только в пределах длины последовательности, видимой на обучении.
- **Контекстуализация с учётом запроса:** Помогает в синтетических задачах, но мало влияет на реальный вопросно-ответный анализ.
- **Тонкая настройка инструкций:** Не является единственной причиной U-образной кривой — базовые модели без дообучения также демонстрируют этот эффект.

---

## Многодокументный вопросно-ответный анализ

**Цель:** Понять, как языковые модели используют входной контекст, изменяя длину и положение релевантной информации.

### Экспериментальная установка

- Вход: вопрос и $k$ документов, где ровно один содержит ответ, остальные — отвлекающие.
- Используем данные NaturalQuestions-Open (Google queries + ответы из Википедии).
- Для каждого вопроса: один релевантный документ + $k-1$ отвлекающих (извлекаются по релевантности, но не содержат ответ).
- Варьируем положение релевантного документа и длину контекста.

**Пример задачи:**

![Пример задачи](figures/qa_example.png)

**Влияние позиции:**

![Положение релевантной информации](figures/qa_changing_position.png)

**Влияние длины:**

![Длина контекста](figures/qa_changing_length.png)

### Модели

- **Открытые:** MPT-30B-Instruct (8192 токена), LongChat-13B (16K токенов)
- **Закрытые:** GPT-3.5-Turbo (4K/16K), Claude-1.3 (8K/100K)
- **Дополнительно:** GPT-4, Flan-UL2, Flan-T5-XXL (кодер-декодер)

### Результаты

![U-образная кривая](figures/qa.png)

- **U-образная кривая:** Модели лучше всего отвечают, когда релевантная информация в начале или в конце контекста.
- **Модели с расширенным контекстом не выигрывают:** Например, GPT-3.5-Turbo и его версия на 16K токенов показывают почти идентичные результаты, если задача помещается в их окно.

**Таблица: Точность в закрытой книге и оракульской настройке**

| Модель              | Закрытая книга | Оракул   |
|---------------------|:-------------:|:--------:|
| LongChat            | 35.0%         | 83.4%    |
| MPT-30B-Instruct    | 31.5%         | 81.9%    |
| GPT-3.5-Turbo       | 56.1%         | 88.3%    |
| GPT-3.5-Turbo (16K) | 56.0%         | 88.6%    |
| Claude-1.3          | 48.3%         | 76.1%    |
| Claude-1.3 (100K)   | 48.2%         | 76.4%    |

---

## Насколько хорошо языковые модели могут извлекать из входных контекстов?

**Синтетическая задача:**  
Модели получают JSON-объект с $k$ парами ключ-значение (UUID) и должны вернуть значение по заданному ключу.

![Пример задачи извлечения ключевых значений](figures/kv_retrieval_example.png)

**Результаты:**

![Результаты извлечения ключевых значений](figures/kv_records.png)

- Claude-1.3 и Claude-1.3 (100K) справляются идеально.
- GPT-3.5-Turbo, GPT-3.5-Turbo (16K), MPT-30B-Instruct — производительность резко падает, если релевантная пара в середине.
- LongChat иногда генерирует не ответ, а код для поиска ключа.

---

## Почему языковые модели неустойчивы к изменению позиции релевантной информации?

### Архитектура модели

- Кодер-декодер (Flan-UL2, Flan-T5-XXL) устойчивы к позиции, но только если длина последовательности не превышает ту, что была на обучении.
- При превышении — снова появляется U-образная кривая.

![Архитектура: сравнение моделей](figures/qa_decoder_only_vs_encoder_decoder.png)

### Контекстуализация с учётом запроса

- Если запрос размещать и до, и после данных, производительность в синтетической задаче становится почти идеальной.
- В реальном вопросно-ответном анализе улучшения минимальны.

![Контекстуализация с учётом запроса](figures/20_total_documents_precondition_with_question.png)

### Тонкая настройка инструкций

- Даже базовые модели (без дообучения инструкциями) демонстрируют U-образную кривую.
- Тонкая настройка слегка уменьшает разницу между лучшей и худшей производительностью.

![Базовая vs дообученная модель](figures/base_vs_instruction_tuned.png)

---

## Всегда ли больше контекста — лучше? (Тематическое исследование)

- При увеличении числа извлечённых документов производительность saturates (насыщается) задолго до насыщения самого извлечения.
- Использование 50 документов вместо 20 даёт лишь незначительный прирост (1–1.5%).

![ODQA: производительность и извлечение](figures/odqa.png)

---

## Связанные работы

- **Модели с длинным контекстом:** Модификации внимания, FlashAttention, Hyena, RWKV и др.
- **Как LLM используют контекст:** Эффект недавности, чувствительность к лингвистическим особенностям, неэффективность при длинных контекстах.
- **Эффект серийной позиции:** В психологии — лучше запоминаются первые и последние элементы списка. В LLM — аналогично.

---

## Заключение

- Производительность языковых моделей существенно зависит от положения релевантной информации в длинном контексте.
- Даже современные модели с большим окном не гарантируют эффективное использование длинного контекста.
- Для будущих моделей с длинным контекстом важно оценивать не только абсолютную производительность, но и устойчивость к позиции релевантной информации.

